import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import re
from scipy import stats
from sklearn.linear_model import LinearRegression

# Generates Figure 4 and 7 from the paper along with several unused graphs, and provides analysis on the data generated by the rq2-rq3-run-harnesses script

plt.rcParams.update({'font.size': 18})

def parse_function_metrics(file_path, repo, targets):
    function_metrics = {}
    with open(file_path, 'r') as file:
        for line in file:
            if "file analyzed" in line:
                break  # Stop parsing after this line

            parts = line.strip().split()
            if len(parts) < 6:  # Ensure the line contains enough fields
                continue

            try:
                # Extract metrics
                nloc = int(parts[0])
                ccn = int(parts[1])
                param = int(parts[3])
                # Extract function name (before '@' symbol)
                func_name = parts[5].split('@')[0]
                func_location = parts[5].split('@')[-1]

                if repo == 'contiki':
                    src_file = func_location.split('/')[-2] + '/' + func_location.split('/')[-1]
                else:
                    src_file = func_location.split('/')[-1]

                # Check for duplicate function name only for target functions
                if func_name in targets or func_name + ':' + src_file in targets:

                    function_metrics[func_name] = {
                        'NLOC': nloc,
                        'CCN': ccn,
                        'Parameters': param,
                    }

            except ValueError:
                # Skip lines that don't match the expected format
                continue

    return function_metrics

def convert_to_milliseconds(time_str):
    match = re.match(r"(\d+)m([\d\.]+)s", time_str)
    if match:
        minutes = int(match.group(1))
        seconds = float(match.group(2))
        return int(minutes * 60000 + seconds * 1000)
    return None

def main():

    # if len(sys.argv) < 2:
    #     print("Usage: plotHarnessData.py <repo name>")
    #     sys.exit(1)

    # repo_name = sys.argv[1]
    # if not repo_name in ['contiki', 'zephyr', 'RIOT']:
    #     print("Invalid repo name")
    #     sys.exit(1)

    # Load funcAnalysis.csv
    func_data = []
    for repo_name in ['contiki', 'zephyr', 'RIOT', 'freeRTOS']:
        repo_data = pd.read_csv(f"./runHarness/results/{repo_name}_funcAnalysis.csv")

        with open(f"./lizardAnalysis/{repo_name}/funcNames.txt", "r") as file:
            lines = file.readlines()
            relevant_functions = {line[:-1] for line in lines}

        # There are copies of some folders in lizard analysis so they match the repo names used here
        metrics = parse_function_metrics(f"./lizardAnalysis/{repo_name}/funcMetrics.txt", repo_name, relevant_functions)
        # Parse funcMetrics.txt

        metrics_df = pd.DataFrame.from_dict(metrics, orient="index").reset_index().rename(columns={"index": "function"})

        # Merge datasets on function name (assuming `FunctionName` in CSV)
        repo_data = repo_data.merge(metrics_df, left_on="Affected Function", right_on="function", how="left")
        print(f"Analyzing {len(repo_data)} functions from {repo_name}")
        repo_data = repo_data.drop_duplicates(subset="Affected Function", keep="first")
        print(f"Running {len(repo_data)} harnesses from {repo_name}")

        func_data.append(repo_data)

    func_data = pd.concat(func_data, ignore_index=True) #Combine data from each repo
    l1 = len(func_data)    
    func_data = func_data[func_data["Success"] != False] #Remove any row with invalid fields
    l2 = len(func_data)
    print(f"Removed {l1 - l2} functions that failed to run")

    # func_data["CBMC Time"] = func_data["CBMC Time"].apply(convert_to_milliseconds) / 1000
    func_data['Total Time'] = func_data['Total Time'].apply(convert_to_milliseconds) / 1000
    print(f"Mean execution time (s): {func_data['Total Time'].mean()}")
    print(f"Percentage of Proofs below 60 seconds: {len(func_data[func_data['Total Time'] <= 60]) / len(func_data['Total Time'])  * 100}")
    print(f"Percentage of Proofs above 3 minutes: {len(func_data[func_data['Total Time'] >= 180])}")
    # Scatter Plot: Unit proof LOC vs. Size of functional unit


    # There are some random functions that successfully complete but don't create coverage reports, so filter them here
    valid_data = func_data[
                       (func_data["Unit Proof LOC"] != -1) & 
                       (func_data["Total Lines (All)"] != -1)]

    # Extract X and y values
    X = valid_data[["Unit Proof LOC"]].values  # Independent variable
    y = valid_data["Total Lines (All)"].values  # Dependent variable
    print(f"Percentage of Proofs below 30 lines of code: {len(valid_data[valid_data['Unit Proof LOC'] <= 30]) / len(valid_data['Unit Proof LOC'])  * 100}")
    print(f"# of Proofs above 50 lines of code: {len(valid_data[valid_data['Unit Proof LOC'] >= 50])}")
    # Fit linear regression
    model = LinearRegression()
    model.fit(X, y)
    r_squared = model.score(X, y)  # Compute R^2
    print(f"R2 for Unit Proof Size vs Functional Unit Size: {r_squared}")

    plt.figure(figsize=(12, 8))
    plt.scatter(valid_data["Unit Proof LOC"], valid_data["Total Lines (All)"], alpha=0.5)
    plt.xlabel("Unit Proof Size (LOC)")
    plt.ylabel("Functional Unit Size (LOC)")
    plt.xlim(0)
    plt.ylim(0)
    # plt.title("Unit Proof NLOC vs. Functional Unit NLOC")
    plt.grid()
    plt.savefig(f'./plots/sizeScatter.png')
    plt.close()

    # CDF of Total Times
    total_times = np.sort(func_data["Total Time"].dropna())
    cdf = np.arange(1, len(total_times) + 1) / len(total_times)

    plt.figure(figsize=(10, 6))
    plt.plot(total_times, cdf, marker='o', linestyle='-', linewidth="3", markersize="7.0")
    plt.xlabel("Proof Execution Time (s)")
    # plt.title("Cumulative Density Function (CDF) of CBMC Execution Times")
    plt.xticks(np.arange(0, total_times.max() + 50, 250))  # Labels in 120s intervals
    plt.ylim(0)
    plt.grid(which='both', axis='y', visible=True)
    plt.savefig(f'./plots/timeCDF.png')
    plt.close()

    target_coverage = np.sort(func_data["Coverage % (Target)"].dropna().str.rstrip('%').astype(float))
    unit_coverage = np.sort(func_data["Coverage % (All)"].dropna().str.rstrip('%').astype(float))
    unit_cdf = np.arange(1, len(unit_coverage) + 1) / len(unit_coverage)
    target_cdf = np.arange(1, len(target_coverage) + 1) / len(unit_coverage)
    
    plt.figure(figsize=(10, 6))
    plt.plot(unit_coverage, unit_cdf, marker='None', linestyle='-', linewidth="3", color="b", label="Functional Unit")
    plt.plot(target_coverage, target_cdf, marker='None', linestyle='--', linewidth="3", color="r", label="Func. Unit Entry Point")
    plt.xlabel("Coverage(%)")
    plt.xticks(np.arange(0, unit_coverage.max() + 10, 10))  # Labels in 120s intervals
    plt.ylim(0)
    plt.grid(which='both', axis='y', visible=True)
    plt.legend(loc="upper left")  # Adding legend
    plt.savefig(f'./plots/coverageCDF.png')
    plt.close()

   # List of factors
    factors = ["# Variables", "# Clauses", "NLOC", "CCN", "Total Lines (All)"]
    graph_names = {
        '# Variables': 'variables',
        '# Clauses': 'clauses',
        'NLOC': 'NLOC',
        'CCN': 'CCN',
        'Total Lines (All)': 'total_lines'
    }
    sorted_data = func_data.sort_values(by="Total Time")

    # Iterate over each factor and plot it in a separate figure
    for factor in factors:
        # Sort the data by the current factor
        sorted_data = func_data[func_data[factor].notna()]
        if(factor == 'Total Lines (All)'):
            sorted_data = sorted_data[sorted_data["Total Lines (All)"] != -1]


        sorted_data = sorted_data.sort_values(by=factor)


        # Extract x and y values
        x = sorted_data["Total Time"]
        y = sorted_data[factor]

        # Calculate Pearson's correlation coefficient and R-squared
        pearson_corr, _ = stats.pearsonr(x, y)
        slope, intercept, r_value, p_value, std_err = stats.linregress(x, y)
        r_squared = r_value ** 2


        # Create a new figure for each factor
        plt.figure(figsize=(10, 6))

        # Plot the sorted data
        plt.plot(x, y, marker='o', linestyle='-', label=f"{factor}", linewidth='3')
        plt.ylabel(factor)
        plt.xlabel("Total Execution Time (s)")
        # plt.title(f"Total Execution Time vs. {factor}")
        plt.grid(True)
        if factor == '# Variables' or factor == '# Clauses':
            plt.yscale('log')


        # Display the Pearson correlation coefficient and R-squared on the plot
        textstr = f"Pearson: {pearson_corr:.4f}\nR-squared: {r_squared:.4f}"
        plt.gca().text(0.95, 0.95, textstr, transform=plt.gca().transAxes, fontsize=12,
            verticalalignment='top', horizontalalignment='right', bbox=dict(facecolor='white', alpha=0.8, edgecolor='black', boxstyle='round,pad=0.5'))

        # Save the figure with the cleaned filename
        plt.savefig(f'./plots/{graph_names[factor]}_lineGraph.png')

        # Close the figure to avoid overlap when creating the next one
        plt.close()

    # Combined variables and NLOC graph
    # Filter and sort data
    valid_data = func_data[func_data["# Variables"].notna() & func_data["Total Lines (All)"].notna()]
    valid_data = valid_data.sort_values(by="Total Time")

    # Extract x and y values
    x = valid_data["Total Time"]
    y_variables = valid_data["# Variables"]
    y_nloc = valid_data["Total Lines (All)"]

    # Compute correlation and R-squared for both factors
    pearson_corr_vars, _ = stats.pearsonr(x, y_variables)
    pearson_corr_nloc, _ = stats.pearsonr(x, y_nloc)
    slope_vars, intercept_vars, r_value_vars, _, _ = stats.linregress(x, y_variables)
    slope_nloc, intercept_nloc, r_value_nloc, _, _ = stats.linregress(x, y_nloc)
    r_squared_vars = r_value_vars ** 2
    r_squared_nloc = r_value_nloc ** 2

    # Create figure and twin axes
    fig, ax1 = plt.subplots(figsize=(10, 6))

    plt.xscale('log')
    # Plot # Variables (left y-axis, log scale)
    ax1.plot(x, y_variables, marker='o', linestyle='-', color='blue', label="# Variables")
    ax1.set_xlabel("Total Execution Time (s)")
    ax1.set_ylabel("Formula Variables", color='blue')
    ax1.set_yscale("log")  # Set log scale for left y-axis
    ax1.tick_params(axis='y', labelcolor='blue')
    

    # Create second y-axis for NLOC (right y-axis, linear scale)
    ax2 = ax1.twinx()
    ax2.plot(x, y_nloc, marker='s', linestyle='dotted', color='red', label="NLOC")
    ax2.set_ylabel("Reachable Lines", color='red')
    ax2.tick_params(axis='y', labelcolor='red')

    # Title and Grid
    # plt.title("Total Execution Time vs. Formula Variables and Reachable Lines")
    ax1.grid(True, which="both", linestyle="-", linewidth=0.5)
    ax1.grid(which="minor", visible=False)

    # Display correlation stats on the plot
    textstr = (f"# Variables: p={pearson_corr_vars:.4f}, R²={r_squared_vars:.4f}\n"
            f"NLOC: p={pearson_corr_nloc:.4f}, R²={r_squared_nloc:.4f}")
    ax1.text(0.20, 0.95, textstr, transform=ax1.transAxes, fontsize=12,
            verticalalignment='top', horizontalalignment='left',
            bbox=dict(facecolor='white', alpha=0.8, edgecolor='black', boxstyle='round,pad=0.5'))

    # Save plot
    plt.savefig('./plots/variables_nloc_dual_axis.png')
    plt.close()

if __name__ == "__main__":
    main()
