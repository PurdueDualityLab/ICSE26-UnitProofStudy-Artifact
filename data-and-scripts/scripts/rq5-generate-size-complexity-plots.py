import statistics
import math
import matplotlib.pyplot as plt
import matplotlib.scale as scl
import seaborn as sns
import numpy as np
import pandas as pd
import argparse
import json

# This script parses through files generated by the lizard analysis tool to create plots comparing size and complexity of vulnerable functions against the rest of the repo
# Input the name of the repo you are interested in generating plots for, or use "all" to generate plots that compare data across all repos
# Used to generate Figure 9 from the paper

parser = argparse.ArgumentParser(description="Parse command line args about OS")
parser.add_argument("filename", help="Name of the file to process")  # Positional argument

plt.rcParams.update({'font.size': 22})

def parse_function_metrics(file_path, root):
    function_metrics = {}
    duplicates_found = set()

    with open(file_path, 'r') as file:
        for line in file:
            if "file analyzed" in line:
                break  # Stop parsing after this line

            parts = line.strip().split()
            if len(parts) < 6:  # Ensure the line contains enough fields
                continue

            try:
                # Extract metrics
                nloc = int(parts[0])
                ccn = int(parts[1])
                token = int(parts[2])
                param = int(parts[3])
                length = int(parts[4])

                # Extract function name (before '@' symbol)
                func_location = parts[5]
                func_name = func_location.split('@')[0]

                # Check for duplicate function name only for target functions
                if func_name in function_metrics or func_name in duplicates_found:
                    if not func_name in duplicates_found:
                        duplicates_found.add(func_name)

                        # The first time we find a duplicate function we don't know about it's duplicants yet
                        # So when we find a dupe, we reformat the original entry accordingly
                        old_metrics = function_metrics[func_name]

                        # Kind of hacky, but most contiki dupes also have duplicate file names so we need more of the path
                        if root == 'contiki-ng':
                            loc = old_metrics["Location"].split('/')[-2] + '/' + old_metrics["Location"].split('/')[-1]
                        else:
                            loc = old_metrics["Location"].split('/')[-1]
                        del function_metrics[func_name]
                        function_metrics[func_name + ':' + loc] = old_metrics
                    if root == 'contiki-ng':
                        file_name = func_location.split('/')[-2] + '/' + func_location.split('/')[-1]
                    else:
                        file_name = func_location.split('/')[-1]
                    func_name = func_name + ":" + file_name


                function_metrics[func_name] = {
                    'NLOC': nloc,
                    'CCN': ccn,
                    'Tokens': token,
                    'Parameters': param,
                    'Length': length,
                    'Location': func_location
                }

            except ValueError:
                # Skip lines that don't match the expected format
                continue

    return function_metrics

def calculate_summary_metrics(function_metrics, summary_metrics_target, root):
    # Collect all metrics for aggregation
    funcs = {
        'root': repo,
        'NLOC': {
            'target': [],
            'max_val': -1,
            'max_func': "",
            'min_val': math.inf,
            'min_func': "",
            'relative_cnt': 0
        },
        'CCN': {
            'target': [],
            'max_val': -1,
            'max_func': "",
            'min_val': math.inf,
            'min_func': "",
            'relative_cnt': 0
        },
        'Tokens': {
            'target': [],
            'max_val': -1,
            'max_func': "",
            'min_val': math.inf,
            'min_func': "",
            'relative_cnt': 0
        },
        'Parameters': {
            'target': [],    
            'max_val': -1,
            'max_func': "",
            'min_val': math.inf,
            'min_func': "",
            'relative_cnt': 0
        },
        'Length': {
            'target': [],
            'max_val': -1,
            'max_func': "",
            'min_val': math.inf,
            'min_func': "",
            'relative_cnt': 0
        },
    }

    for func, metrics in function_metrics.items():
        for metric, value in metrics.items(): #Exclude location
            if(metric != 'Location'):
                if(value > funcs[metric]["max_val"]):
                    funcs[metric]["max_val"] = value
                    funcs[metric]["max_func"] = func
                elif(value < funcs[metric]["min_val"]):
                    funcs[metric]["min_val"] = value
                    funcs[metric]["min_func"] = func

                if(value <= summary_metrics_target[metric]["Max"]):
                    funcs[metric]["relative_cnt"] += 1

                funcs[metric]['target'].append(value)

    funcs['repo'] = root

    # Calculate summary statistics
    summary_metrics = {
        'NLOC': {
            'Mean': sum(funcs['NLOC']['target']) / len(funcs['NLOC']['target']),
            'StdDev': statistics.stdev(funcs['NLOC']['target']),
            'Max': funcs['NLOC']['max_val'],
            'MaxFunc': funcs['NLOC']['max_func'],
            'Min': funcs['NLOC']['min_val'],
            'MinFunc': funcs['NLOC']['min_func'],
            'RelativeToTarget': f"{round(funcs['NLOC']['relative_cnt'] / len(funcs['NLOC']['target']) * 100, 4)}%"
        },
        'CCN': {
            'Mean': sum(funcs['CCN']['target']) / len(funcs['CCN']['target']),
            'StdDev': statistics.stdev(funcs['CCN']['target']),
            'Max': funcs['CCN']['max_val'],
            'MaxFunc': funcs['CCN']['max_func'],
            'Min': funcs['CCN']['min_val'],
            'MinFunc': funcs['CCN']['min_func'],
            'RelativeToTarget': f"{round(funcs['CCN']['relative_cnt'] / len(funcs['CCN']['target']) * 100, 4)}%"
        },
        'Tokens': {
            'Mean': sum(funcs['Tokens']['target']) / len(funcs['Tokens']['target']),
            'StdDev': statistics.stdev(funcs['Tokens']['target']),
            'Max': funcs['Tokens']['max_val'],
            'MaxFunc': funcs['Tokens']['max_func'],
            'Min': funcs['Tokens']['min_val'],
            'MinFunc': funcs['Tokens']['min_func'],
            'RelativeToTarget': f"{round(funcs['Tokens']['relative_cnt'] / len(funcs['Tokens']['target']) * 100, 4)}%"           
        },
        'Parameters': {
            'Mean': sum(funcs['Parameters']['target']) / len(funcs['Parameters']['target']),
            'StdDev': statistics.stdev(funcs['Parameters']['target']),
            'Max': funcs['Parameters']['max_val'],
            'MaxFunc': funcs['Parameters']['max_func'],
            'Min': funcs['Parameters']['min_val'],
            'MinFunc': funcs['Parameters']['min_func'],
            'RelativeToTarget': f"{round(funcs['Parameters']['relative_cnt'] / len(funcs['Parameters']['target']) * 100, 4)}%"
        },
        'Length': {
            'Mean': sum(funcs['Length']['target']) / len(funcs['Length']['target']),
            'StdDev': statistics.stdev(funcs['Length']['target']),
            'Max': funcs['Length']['max_val'],
            'MaxFunc': funcs['Length']['max_func'],
            'Min': funcs['Length']['min_val'],
            'MinFunc': funcs['Length']['min_func'],
            'RelativeToTarget': f"{round(funcs['Length']['relative_cnt'] / len(funcs['Length']['target']) * 100, 4)}%"
        }
    }

    return summary_metrics, funcs


def calculate_summary_metrics_for_target(function_metrics, target_functions, root):
    # Collect all metrics for aggregation for target functions
    
    funcs = {
        'repo': root,
        'NLOC': {
            'target': [],
            'max_val': -1,
            'max_func': "",
            'min_val': math.inf,
            'min_func': ""
        },
        'CCN': {
            'target': [],
            'max_val': -1,
            'max_func': "",
            'min_val': math.inf,
            'min_func': ""
        },
        'Tokens': {
            'target': [],
            'max_val': -1,
            'max_func': "",
            'min_val': math.inf,
            'min_func': ""
        },
        'Parameters': {
            'target': [],    
            'max_val': -1,
            'max_func': "",
            'min_val': math.inf,
            'min_func': ""
        },
        'Length': {
            'target': [],
            'max_val': -1,
            'max_func': "",
            'min_val': math.inf,
            'min_func': ""
        },
    }

    for func in target_functions:
        if func in function_metrics:
            metrics = function_metrics[func]
            for metric, value in metrics.items(): #Exclude location
                if(metric != 'Location'):
                    if(value > funcs[metric]["max_val"]):
                        funcs[metric]["max_val"] = value
                        funcs[metric]["max_func"] = func
                    elif(value < funcs[metric]["min_val"]):
                        funcs[metric]["min_val"] = value
                        funcs[metric]["min_func"] = func
                    funcs[metric]['target'].append(value)

        else:
            print(f"Error: {func} not found in metrics data, excluding from analysis")



    summary_metrics_target = {
        'NLOC': {
            'Mean': sum(funcs['NLOC']['target']) / len(funcs['NLOC']['target']),
            'StdDev': statistics.stdev(funcs['NLOC']['target']),
            'Max': funcs['NLOC']['max_val'],
            'MaxFunc': funcs['NLOC']['max_func'],
            'Min': funcs['NLOC']['min_val'],
            'MinFunc': funcs['NLOC']['min_func'],
        },
        'CCN': {
            'Mean': sum(funcs['CCN']['target']) / len(funcs['CCN']['target']),
            'StdDev': statistics.stdev(funcs['CCN']['target']),
            'Max': funcs['CCN']['max_val'],
            'MaxFunc': funcs['CCN']['max_func'],
            'Min': funcs['CCN']['min_val'],
            'MinFunc': funcs['CCN']['min_func'],
        },
        'Tokens': {
            'Mean': sum(funcs['Tokens']['target']) / len(funcs['Tokens']['target']),
            'StdDev': statistics.stdev(funcs['Tokens']['target']),
            'Max': funcs['Tokens']['max_val'],
            'MaxFunc': funcs['Tokens']['max_func'],
            'Min': funcs['Tokens']['min_val'],
            'MinFunc': funcs['Tokens']['min_func'],
        },
        'Parameters': {
            'Mean': sum(funcs['Parameters']['target']) / len(funcs['Parameters']['target']),
            'StdDev': statistics.stdev(funcs['Parameters']['target']),
            'Max': funcs['Parameters']['max_val'],
            'MaxFunc': funcs['Parameters']['max_func'],
            'Min': funcs['Parameters']['min_val'],
            'MinFunc': funcs['Parameters']['min_func'],
        },
        'Length': {
            'Mean': sum(funcs['Length']['target']) / len(funcs['Length']['target']),
            'StdDev': statistics.stdev(funcs['Length']['target']),
            'Max': funcs['Length']['max_val'],
            'MaxFunc': funcs['Length']['max_func'],
            'Min': funcs['Length']['min_val'],
            'MinFunc': funcs['Length']['min_func'],
        }
    }
    return summary_metrics_target, funcs

def plot_function_metrics(all_funcs, target_funcs, root):
    # Create separate plots for Tokens, Parameters, and Length metrics
    plt.rcParams.update({'font.size': 18})  
    for metric in ['Tokens', 'Parameters', 'Length']:
        plt.figure(figsize=(20, 10))
        
        all_func_data = np.concatenate([repo[metric]['target'] for repo in all_funcs])
        target_func_data = np.concatenate([repo[metric]['target'] for repo in target_funcs])
        
        df = pd.DataFrame({
            'value': np.concatenate([all_func_data, target_func_data]),
            'distribution': ['All'] * len(all_func_data) + ['Target'] * len(target_func_data),
            'repo': np.concatenate([
                np.concatenate([[repo['repo'] for _ in range(len(repo[metric]['target']))] for repo in all_funcs]),
                np.concatenate([[repo['repo'] for _ in range(len(repo[metric]['target']))] for repo in target_funcs])
            ])
        })

        # Define the threshold at the 99th percentile
        percentile_99_all = df[df['distribution'] == 'All']['value'].quantile(0.997)
        percentile_99_target = df[df['distribution'] == 'Target']['value'].quantile(0.95)
        
        # Remove values above this percentile
        df_filtered = df[(df['distribution'] == 'Target') & (df['value'] <= percentile_99_target) | 
                        ((df['distribution'] == 'All') & (df['value'] <= percentile_99_all))]
        
        # Create the violin plot
        ax = sns.violinplot(data=df_filtered, x='repo', y='value', split=True, inner='quart', 
                           hue='distribution', common_norm=True, cut=0, width=1.0)
        handles, labels = ax.get_legend_handles_labels()
        new_labels = [f"{label} (n={df[df['distribution'] == label].shape[0]})" for label in labels]

        plt.legend(handles, new_labels, title="Sample Sizes")
        # Add labels and title
        plt.title(f"{metric} Distributions")
        if(root == 'all'):
            plt.savefig(f'./{metric}_plot.png')
        else:
            plt.savefig(f'./{root}/{root}_{metric}_plot.png')
        plt.clf()
    
    # Create a combined figure for NLOC and CCN
    fig, axes = plt.subplots(2, 1, figsize=(18, 14), sharex=True)
    
    # Process NLOC and CCN as subplots
    for idx, metric in enumerate(['NLOC', 'CCN']):
        all_func_data = np.concatenate([repo[metric]['target'] for repo in all_funcs])
        target_func_data = np.concatenate([repo[metric]['target'] for repo in target_funcs])
        
        df = pd.DataFrame({
            'value': np.concatenate([all_func_data, target_func_data]),
            'distribution': ['All'] * len(all_func_data) + ['Target'] * len(target_func_data),
            'repo': np.concatenate([
                np.concatenate([[repo['repo'] for _ in range(len(repo[metric]['target']))] for repo in all_funcs]),
                np.concatenate([[repo['repo'] for _ in range(len(repo[metric]['target']))] for repo in target_funcs])
            ])
        })

        # Define the threshold at the 99th percentile
        percentile_99_all = df[df['distribution'] == 'All']['value'].quantile(0.997)
        percentile_99_target = df[df['distribution'] == 'Target']['value'].quantile(0.95)
        
        # Remove values above this percentile
        df_filtered = df[(df['distribution'] == 'Target') & (df['value'] <= percentile_99_target) | 
                        ((df['distribution'] == 'All') & (df['value'] <= percentile_99_all))]
        
        # Create the violin plot on the current subplot
        ax = sns.violinplot(data=df_filtered, x='repo', y='value', split=True, inner='quart', 
                           hue='distribution', common_norm=True, cut=0, width=1.0, ax=axes[idx])
        
        handles, labels = ax.get_legend_handles_labels()
        new_labels = [f"{label} (n={df_filtered[df_filtered['distribution'] == label].shape[0]})" for label in labels]
        ax.legend(handles, new_labels, title="Sample Sizes", fontsize=12)
        
        # Add labels and title for the subplot
        ax.set_ylabel(metric)
    
    # Add a common title for the figure
    # plt.suptitle("Code Size (NLOC) and Complexity (CCN) Comparison", fontsize=20)
    plt.tight_layout()
    
    # Save the combined figure
    if(root == 'all'):
        plt.savefig('./NLOC_CCN_combined_plot.png')
    else:
        plt.savefig(f'./{root}/{root}_NLOC_CCN_combined_plot.png')
    plt.close()

def print_function_metrics(file_path, target_funcs_file, root):
    function_metrics = parse_function_metrics(file_path, root)
    
    # Read target function names from the second file
    with open(target_funcs_file, 'r') as file:
        target_functions = set(line.strip() for line in file)

    # Calculate and print the summary metrics for target functions
    summary_metrics_target, target_funcs = calculate_summary_metrics_for_target(function_metrics, target_functions, root)
    if summary_metrics_target:
        print(f"\nNum target functions evaluated: {len(target_funcs['NLOC']['target'])}")
        print("\nSummary Metrics for target functions:")
        for metric, values in summary_metrics_target.items():
            print(f"\n{metric}:")
            print(f"  Mean: {values['Mean']}")
            print(f"  StdDev: {values['StdDev']}")
            print(f"  Max: {values['Max']}")
            print(f"  Max Func: {values['MaxFunc']}")
            print(f"  Min: {values['Min']}")
            print(f"  Min Func: {values['MinFunc']}")
    else:
        print("\nNo target functions found or no data available for the target functions.")


    # Calculate summary statistics for all functions
    summary_metrics_all, all_funcs = calculate_summary_metrics(function_metrics, summary_metrics_target, root)

    # Print the compiled summary statistics for all functions
    print(f"\nNum target functions evaluated: {len(all_funcs['NLOC']['target'])}")
    print("\nSummary Metrics for all functions:")
    for metric, values in summary_metrics_all.items():
        print(f"\n{metric}:")
        print(f"  Mean: {values['Mean']}")
        print(f"  StdDev: {values['StdDev']}")
        print(f"  Max: {values['Max']}")
        print(f"  Max Func: {values['MaxFunc']}")
        print(f"  Min: {values['Min']}")
        print(f"  Min Func: {values['MinFunc']}")
        print(f"  RelativeToTarget: {values['RelativeToTarget']}")

    return all_funcs, target_funcs


args = parser.parse_args()
if args.filename == 'all':
    all_funcs_by_repo = []
    all_targets_by_repo = []
    for repo in ['zephyr', 'RIOT', 'contiki-ng', 'FreeRTOS-Plus-TCP']:
        func_metrics_file = repo + "/funcMetrics.txt"
        target_funcs_file = repo + "/funcNames.txt"
        all_funcs, target_funcs = print_function_metrics(func_metrics_file, target_funcs_file, repo)
        all_funcs_by_repo.append(all_funcs)
        all_targets_by_repo.append(target_funcs)
        with open(f"{repo}/results.json", mode="w", encoding='utf-8') as file:
            json.dump(target_funcs, file, indent=4)
    plot_function_metrics(all_funcs_by_repo, all_targets_by_repo, args.filename)

else:
    func_metrics_file = args.filename + "/funcMetrics.txt"
    target_funcs_file = args.filename + "/funcNames.txt"
    repo = args.filename
    all_funcs, target_funcs = print_function_metrics(func_metrics_file, target_funcs_file, args.filename)
    plot_function_metrics([all_funcs], [target_funcs], args.filename)
    with open("results.json", mode="w", encoding='utf-8') as file:
        json.dump(target_funcs, file, indent=4)